<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta name="renderer" content="webkit">
        <link rel="icon" sizes="192x192" href="assets/next.jpeg">
        <title>TAT-DQA: Towards Complex Document Understanding By Discrete Reasoning</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="TAT-DQA: A Document Visual Question Answering (VQA) Dataset, aiming to answer questions over visually-rich documents with a hybrid of Tabular and Textual Content in Finance">
        <meta name="author" content="Fengbin Zhu, Wenqiang Lei, Chao Wang, Fuli Feng">
        <link href="styles/bootstrap.min.css" rel="stylesheet">
        <link href="styles/common.css" rel="stylesheet">
    </head>

    <body data-spy="scroll" data-target=".navbar">
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <h2 class="navbar-brand hidden-xs hidden-sm">
                        <span class="logo">TAT-DQA</span>
                        <span class="description">VQA Challenge over Table-Text Documents</span>
                    </h2>

                    <h2 class="navbar-brand hidden-md hidden-lg">
                        <span class="logo">TAT-DQA</span>
                    </h2>
                </div>

                <div class="collapse navbar-collapse" id="navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li><a class="page-scroll" href="#introduction">Introduction</a></li>
                        <li><a class="page-scroll" href="#start">Getting Started</a></li>
                        <li><a class="page-scroll" href="#leaderboard">Leaderboard</a></li>
                        <li><a class="page-scroll" href="#submission">Submission</a></li>
                        <li><a class="page-scroll" href="#contact">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- linked this part-->
        <section id="introduction" class="scrollable-section">
            <div class="container">
                <h3>Introduction</h3>
                <p>
                    <b>TAT-DQA</b> is a large-scale Document VQA dataset, which is constructed by extending the <a href="https://nextplusplus.github.io/TAT-QA/" target="_blank" ><b>TAT-QA</b></a>.
                    It aims to stimulate progress of QA research over more complex and realistic visually-rich documents with rich tabular and textual content, especially those requiring numerical reasoning.
                </p>
                <p>The unique features of TAT-DQA include:</p>

                <ul>
                    <li>The documents in TAT-DQA dataset are sampled from <ins>real-world high-quality financial reports</ins> and each document contains both <ins>tabular and textual</ins> data; </li>
                    <li>The average number of words of each document in TAT-DQA is around <ins>550</ins>, which is significantly larger than all existing Document VQA datasets. </li>
                    <li>Around <ins>85%</ins> of the documents in the dataset have only one page while <ins>15%</ins> has multiple pages. </li>
                    <li>Similar to TAT-QA, the answer forms are diverse, including <ins>single span</ins>,  <ins>multiple spans</ins> and <ins>free-form</ins> and various numerical reasoning capabilities are usually required, including  <ins>addition (+)</ins>, <ins>subtraction (-)</ins>, <ins>multiplication (x)</ins>, <ins>division (/)</ins>, <ins>counting</ins>, <ins>comparison</ins>, <ins>sorting</ins>, and <ins>their compositions</ins>;</li>
                </ul>
                <p>
                    In total, TAT-DQA contains <b>16,558</b> questions associated with <b>2,758</b> documents ( <b>3,067</b> document pages ) sampled from real-world financial reports.
               </p>
                <p>
                    The following is an example of TAT-DQA. The left is a given visually-rich document. The right are a question associated with the document, the derivation to compute the answer value, the scale of the numerical answer and the final answer.
                </p>

                <img src="assets/tatdqa-sample.png"  alt="TAT-DQA Sample" width="90%" >
                <br>
                   
                For more information, please read our ACM MM 2022 paper  <a href="https://arxiv.org/pdf/2207.11871.pdf" target="_blank">[PDF]</a>.
            </div>
        </section>

        <section id="start" class="scrollable-section">
            <div class="container">
                <h3>Getting Started</h3>

                Download the TAT-DQA Dataset from <a href="https://drive.google.com/drive/folders/1SGpZyRWqycMd_dZim1ygvWhl5KdJYDR2" target="_blank">Google Drive</a>. The data have three parts: the original documents in PDF format, the converted content from the PDF in JSON format and the QA pairs in JSON format. Following are the detailed explainations about the data. 


<br>
<li><ins>PDF documents</ins>:  which is obtained by filtering the document pages corresponding to the QA pairs from the raw financial reports, e.g.,<i> 11ba155b7577c83fe5f3c4f766039e93.pdf</i>. A PDF document in TAT-DQA may contain at most three pages. </li>

<br>
<li><ins>Converted content from PDF in JSON</ins> : obtained by converting the text with a bounding box using a PDF Reader for the readable PDF files or a commercial OCR engine for the images e.g., <i>11ba155b7577c83fe5f3c4f766039e93.json</i>. </li>

<pre>
{
  "pages": [                                                                  <em class="comment"># The document content are stored by page, each page will be one element in this array </em>
    {
      "bbox": [                                                               <em class="comment"># The dimension of this document page, [x1, y1, x2, y2]. (x1, y1) is the top left of the page while (x2, y2) is the bottom right  </em>
        0,
        0,
        1239,
        1754
      ],
      "blocks": [                                                             <em class="comment"># The document content are stored by blocks for each page </em>
        {
          "bbox": [
            34,
            50,
            174,
            71
          ],
          "uuid": "8f1e47dc-af67-485d-b269-057f06e8714c",                     <em class="comment"># The unique ID of the block  </em>
          "text": "Table of Contents",                                        <em class="comment"># The text content of the block  </em>
          "words": {                                                          <em class="comment"># The word list of the content for this block  </em>
            "word_list": [
              "Table",
              "of",
              "Contents"
            ],
            "bbox_list": [                                                    <em class="comment"># The corresponding bounding box of each word in the word list  </em>
              [
                34,
                50,
                75,
                71
              ],
              [
                84,
                50,
                100,
                71
              ],
              [
                108,
                50,
                174,
                71
              ]
            ]
          },
          "order": 1                                                        <em class="comment"># The order of the block in this page, starting from 1 </em>
        },
        ...
    ]
   }
  ]
 }
</pre>

<li> <ins>QA Pairs</ins>: The QA pairs associated to the given document. </li>
<pre>
{ 
  "doc": {
    "uid": "11ba155b7577c83fe5f3c4f766039e93",                                      <em class="comment"># The unique id of the document, {uid}.pdf and {uid}.json are the given PDF document and the OCR result of the PDF </em>
    "page" : 1,                                                                     <em class="comment"># The starting page no. of the document that is relevant to the QA pairs</em>
    "source": "navios-maritime-holdings-inc_2019.pdf"                               <em class="comment"># The financial statement that this doc comes from </em>
  },                   
  "questions": [                                                                    <em class="comment"># The questions associated to the given document </em>
  {                   
    "uid": "9dfd4a8d0b9e91c2ab5ab4a8745226c6",                                      <em class="comment"># The unique id of a question</em>
    "order": 6,                                                                     <em class="comment"># The order of the question in all questions, starting from 1</em>
    "question": "What was the change in Impairment losses between 2017 and 2018?",  <em class="comment"># The question itself</em>
    "answer": 150092,                                                               <em class="comment"># The ground-truth answer</em>
    "derivation": "200,657-50,565",                                                 <em class="comment"># The derivation that can be executed to arrive at the ground-truth answer</em>
    "answer_type": "arithmetic",                                                    <em class="comment"># The answer type including `span`, `spans`, `arithmetic` and `counting`.</em>
    "scale": "thousand",                                                            <em class="comment"># The scale of the answer including `None`, `thousand`, `million`, `billion` and `percent`</em>
    "req_comparison": false,                                                        <em class="comment"># A flag indicating if `comparison/sorting` is needed to answer the question whose answer is a single span or multiple spans</em>
    "facts": [                                                                      <em class="comment">#[Optional] The facts or evidencies used to infer the final answers, which are generated heuristically  </em>
      "200,657",                     
      "50,565"                     
    ],                     
    "block_mapping": [                                                              <em class="comment"># The facts' positions in the OCR result of the given document. </em>
      {
        "69352448-14ad-4854-87a9-3ac44358a660": [
          26,
          32
        ]
      },
      {
        "69352448-14ad-4854-87a9-3ac44358a660": [
          18,
          25
        ]
      }
    ]
    ...                                                 
  ]
}
</pre>

            </div>
        </section>

        <section id="leaderboard" class="scrollable-section">
            <div style="background-color: #e8e8e8">
                <div class="container">
                    <h3>Leaderboard</h3>

                    <div class="table-responsive">
                    <table class="table well">
                        <thead class="thead-dark">

                        <tr>
                            <th>Rank</th>
                            <th>Model Name</th>
                            <th>Team Name</th>
                            <th>Exact Match</th>
                            <th>F1</th>
                            <th>Created</th>
                            <th>Paper</th>
                            <th>Codes</th>
                        </tr>
                        </thead>

                        <tbody>
                        <tr>
                            <td>-</td>
                            <td>Human Performance</td>
                            <td>-</td>
                            <td>84.1</td>
                            <td>90.8</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                         <tr>
                            <td>1</td>
                            <td>TAT-LLM (70B)</td>
                            <td>NExT</td>
                            <td><b>76.5</b></td>
                            <td><b>83.9</b></td>
                            <td>20 Jan 2024</td>
                            <td><a href="https://arxiv.org/pdf/2401.13223.pdf" target="_blank">Paper</a> </td>
                            <td>N.A.</td></td>
                        </tr>
                          <tr>
                            <td>2</td>
                            <td>TAT-LLM (13B)</td>
                            <td>NExT</td>
                            <td><b>72.2</b></td>
                            <td><b>80.5</b></td>
                            <td>20 Jan 2024</td>
                            <td><a href="https://arxiv.org/pdf/2401.13223.pdf" target="_blank">Paper</a> </td>
                            <td>N.A.</td></td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>TAT-LLM (7B)</td>
                            <td>NExT</td>
                            <td><b>71.3</b></td>
                            <td><b>80.2</b></td>
                            <td>20 Jan 2024</td>
                            <td><a href="https://arxiv.org/pdf/2401.13223.pdf" target="_blank">Paper</a> </td>
                            <td>N.A.</td></td>
                        </tr>
                          <tr>
                            <td>1</td>
                            <td>Doc2SoarGraph</td>
                            <td>NExT</td>
                            <td>59.2</td>
                            <td>67.6</td>
                            <td>10 Jan 2023</td>
                            <td><a href="https://arxiv.org/pdf/2305.01938.pdf" target="_blank">Paper</a> </td>
                            <td><a href="https://github.com/fengbinzhu/Doc2SoarGraph" target="_blank">Codes</a> </td>
                        </tr>
                        <tr>
                            <td>2</td>
                            <td>Baseline - MHST</td>
                            <td>NExT</td>
                            <td>41.5</td>
                            <td>50.7</td>
                            <td>10 Aug 2022</td>
                            <td><a href="https://arxiv.org/pdf/2207.11871.pdf" target="_blank">Paper</a> </td>
                            <td>N.A.</td>
                        </tr>
                        </tbody>
                    </table>
                    </div>

                </div>
            </div>
        </section>

        <section id="submission" class="scrollable-section">
            <div class="container">
                <h3>Submission</h3>
                <p>
                To evaluate your models, we have also made available the evaluation script we will use for official evaluation,
                To run the evaluation, use
                </p>
                <pre>python tatqa_eval.py --gold_path=:path_to_dev --pred_path=:path_to_predictions</pre>

                <h4>Predictions Format</h4>
                <p> The predictions file in JSON format contains a dictionary with question ids as keys and the predictions as values
                    (each prediction shall include both `answer` and `scale` in an array). For example,
                </p>
                <pre>
{
 "9337c3e6-c53f-45a9-836a-02c474ceac16": [
    "4.6",
    "percent"
  ],
  "c4170232-e89c-487a-97c5-afad45e9d702": [
    "16",
    "thousand"
  ],
  "d81d1ae7-363c-4b47-8eea-1906fef33856": [
    ["2018", "2019"],
    ""
  ]
  ...
}
</pre>
                <p>We also provide a <a href="https://github.com/NExTplusplus/tat-qa/blob/master/sample_prediction.json" target="_blank" >sample prediction file </a> (on Dev) for your reference.</p>

                <pre>python tatqa_eval.py --gold_path=dataset_raw/tatqa_dataset_dev.json --pred_path=sample_prediction.json</pre>
                <h4>Submission</h4>
                <p>
                    Please email the prediction file of the test set with the following information to us:
                    <ul>
                        <li>Model name with a brief description of your model</li>
                        <li>Paper title & link (if any)</li>
                        <li>Team name</li>
                        <li>Contact person</li>
                        <li>Email of the contact person</li>
                    </ul>
                </p>
                <p>
                Please give us up to two weeks to evaluate your submission and we will add your model to the leaderboard.
                </p>
            </div>
        </section>

        <section id="contact" class="scrollable-section">
            <div class="container">
                <h3>Contact</h3>
                <p>
                    The TAT-DQA dataset is under the license of <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons (CC BY) Attribution 4.0 International.</a>  
                </p>  
                <p>
                    For more information, please contact:
                    <ul>
                        <li>Fengbin ZHU <a href="mailto:fengbinzhu@u.nus.edu">fengbinzhu@u.nus.edu</a></li>
                    </ul>
                </p>
                <p>
                    Please kindly cite our work if the dataset helps your research.
                </p>
<pre>

@inproceedings{zhu-etal-2021-tat,
    title = "{TAT}-{QA}: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance",
    author = "Zhu, Fengbin  and
      Lei, Wenqiang  and
      Huang, Youcheng  and
      Wang, Chao  and
      Zhang, Shuo  and
      Lv, Jiancheng  and
      Feng, Fuli  and
      Chua, Tat-Seng",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.254",
    doi = "10.18653/v1/2021.acl-long.254",
    pages = "3277--3287"
}

@misc{zhu2022towards,
  doi = {10.48550/ARXIV.2207.11871},
  url = {https://arxiv.org/abs/2207.11871},
  author = {Zhu, Fengbin and Lei, Wenqiang and Feng, Fuli and Wang, Chao and Zhang, Haozhou and Chua, Tat-Seng},
  title = {Towards Complex Document Understanding By Discrete Reasoning},
  publisher = {arXiv},
  year = {2022}
}

</pre>
            </div>
            </div>
        </section>

        <footer class="footer" style="padding-top: 50px;">
            <div class="container">
                <hr>
                <p style="font-style: italic; text-align: center">
                    Copyright &copy; 2018-2022 NExT++ /
                    <a class="black" href="http://www.nextcenter.org/privacy-policy" target="_blank" >Privacy Policy</a> /
                    <a class="black" href="http://www.nextcenter.org/terms-conditions" target="_blank" >Terms &amp; Conditions</a>
                </p>
            </div>
        </footer>

        <script type="text/javascript" src="scripts/jquery-3.2.1.min.js"></script>
        <script type="text/javascript" src="scripts/bootstrap.min.js"></script>
        <script type="text/javascript" src="scripts/jquery.easing.min.js"></script>
        <script type="text/javascript" src="scripts/scrolling-nav.js"></script>
    </body>
</html>
